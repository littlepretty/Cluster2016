\section{Design of Cerberus}
\label{Sec:Scheduler}

\begin{figure}[htp]
\centering
        \includegraphics[width=3.6in]{3PhaseJobFSM}
        \caption{Job state transition in Cerberus}
\label{Fig:JobFSM}
\end{figure}

\subsection{Three-Phase Job Scheduling}


%\NOTE{This paragraph moved to start of Section 3}The traditional batch scheduler on a system without burst buffer only considers
%the job with the number of required nodes and the expected runtime for the scheduling decision making process.
%The most commonly used scheduling policy is the First Come First Serve with EASY backfilling~\cite{tsafrir-tpds-2007}.
%However, on burst-buffer-enabled systems,
%the amount of available burst buffer capacity becomes a new scheduling constraint.
%We propose Cerberus, a burst-buffer-aware batch scheduler.
%Cerberus takes the advantage of the most common use cases of burst buffer:
%data stage-in, checkpointing, and data drain-out.
%Cerberus allocates burst buffer in each phase for one of the three use cases
%by maintaining three distinct queues, as shown in Figure~\ref{Fig:CerberusQueues}.
%%Before entering each phase, job must enqueue to the corresponding queue.
%The input queue $Q_I$ contains all the jobs that need to pre-fetch data from the external storage to the burst buffer.
%The running queue $Q_R$ holds the jobs that are waiting for the compute nodes and the burst buffer used for checkpointing.
%Jobs waiting to be drained out to the external permanent storage are in the output queue $Q_O$.
%The layered hierarchy in Figure~\ref{Fig:CerberusQueues} also indicates that burst buffer
%can fill in the memory gap between the memory on compute nodes and the hard disk storage.

As shown in Figure 1, Cerberus adopts a three-phase job scheduling strategy. 
In order to help understand Cerberus design, we illustrate job state transition in Figure 2.
To see how Cerberus schedules a particular job by coordinating three queues,
we present the job transition diagram in Figure~\ref{Fig:JobFSM}.
After the submission, a job enters $Q_I$ waiting for the stage-in-purpose burst buffer ($BB_{in}$).
Once selected by Cerberus, the job is removed from $Q_I$ and start to stage in data.
After finishing the stage-in phase, the job is enqueued to $Q_R$,
where it waits for the required compute nodes and the \textit{new} burst buffer for checkpointing purpose ($BB_{run}$).
Note that $BB_{in}$ still held by the job at this time point. When Cerberus chooses the job from $Q_R$, the job is in the running phase.
The first thing is to release $BB_{in}$ after \textbf{fetching in} data to memory of the acquired compute nodes.
During the execution, checkpointing and application restart may occur frequently,
which essentially means frequent data exchanges between the memory and the burst buffer $BB_{run}$.
Once the job execution finishes, $BB_{run}$ can be immediately released since
output data are still available on the compute nodes.
Without releasing the compute nodes, the job enters $Q_O$ and waits for a certain amount of burst buffer for staging out its data ($BB_{out}$).
Once a job is chosen by Cerberus, its data flow from the main memory to the burst buffer
in the \textbf{drain-out} phase, after which all the compute nodes can be released;
then the burst buffer nodes are responsible for transferring the output data to the external storage.
The job completes when all its data are staged out and $BB_{out}$ are returned to system.
As indicated by Figure~\ref{Fig:JobFSM}, the resource release at anytime can trigger Cerberus
to schedule jobs in those three queues.

The three-phase job model divides the jobs to generic phases with very specific resource demands.
Cerberus conquers the scheduling problem in each phase, and the detailed decision making is described in Section~\ref{SubSec:OptStageIn}-\ref{SubSec:OptStageOut}.
%The fine scheduling granularity enables the full utilization of burst buffer for different purposes.
%The divide-conquer approach provides a framework that can be extended to various scheduling
%policies for each individual phase.
%We propose and solve two optimizations targeting $Q_I/Q_O$ and $Q_R$ in Section~\ref{Sec:Opt}

\subsection{Three-Phase Coordination}
The three phases together with the corresponding queues, are not entirely independent.
First, only after finishing the stage-in phase can a job be ready for running;
similarly, only when finishing the running phase can a job waiting or entering the stage-out phase.
This is reflected in the lifetime of a job's execution.

Secondly,  the burst buffer demand of any job before entering any phase can only be used for a single purpose.
For example, a running-phase job cannot hold its already taken burst buffer
and to enter stage-out phase upon finishing the running, regardless whether the holding burst buffer has sufficient capacity.
In Cerberus, the resource release is mandatory as specified in Figure~\ref{Fig:JobFSM}.
We believe that frequent releases provide more opportunities to involve the scheduler in each phase for scheduling the jobs.

At last, it is possible that all the three queues are not empty at the moment of scheduling.
We choose a heuristic strategy to determine the scheduling order when
Cerberus has to handle multiple queues at the same time.
Note that jobs finishing the \textit{running} and the \textit{stage-out} phases
will (partially) release the resources, and we set the priority of queues to be $priority(Q_O) > priority(Q_R) > priority(Q_I)$.
This greedy heuristic works well when the demand of the jobs in $J_{Q_I}$ is low.
However, it is possible to make jobs stacking up at the entry of the system.
We plan to investigate this interesting problem in the future.



\subsection{Scheduling in the Stage-in Phase}
\label{SubSec:OptStageIn}

In the stage-in phase, jobs require only the burst buffer.
Cerberus schedules the jobs in $Q_I$ based on their burst buffer demands $bb\_in$.
To maximize burst buffer's utilization, one can schedule the jobs according to the following optimization:
\begin{align*}
        \mathbf{Problem} & \text{ } \mathbf{1} \\
        \max &\sum_{i\in J_{Q_I}} bb\_in_i \cdot x_i \\
        s.t. &\left\{
                \begin{array}{l}
                        x_i \in \{0, 1\}, \forall i \in J_{Q_I}\\ [0.8em] \numberthis \label{Equ:MaxTransferData}
                        \sum_{i \in J_{Q_I}} bb\_in_i \cdot x_i \leq BB_{available}
                \end{array}
        \right.
\end{align*}
%\begin{align*}
        %& \max \sum_{i\in S} bb\_in_i \\
        %& s.t. \sum_{i\in S} bb\_in_i \leq BB_{available} \numberthis \label{Equ:MaxTransferData}
        %%\left\{
                %%\begin{array}{l}
                        %%S \subseteq J_{Q_I} \\ [1em]
                        %%\sum_{i\in S} bb\_in_i \leq BB_{available}
                %%\end{array} 
        %%\right.
%\end{align*}
where $J_{Q_I}$ stands for all jobs in the input queue,
$bb\_in_i$ is the burst buffer demand of $job_i$,
$BB_{available}$ is the system's current available capacity of the burst buffer.
We denote the set of jobs that achieve the maximum data transfer throughput as
$\mathcal{J} = \{x_i|x_i = 1, \text{ } \forall i \in J_{Q_I}\}$.

It is trivial to show that the optimization of Problem 1
is equivalent to the 0-1 knapsack problem, which is NP-hard.
We use dynamic programming to solve it in pseudo-polynomial time.
Memorization technique is applied in order to accelerate the solving process.
The recursive relationship used in our solution is given by \equref{Equ:MaxTransferDataRecursion}.
The time complexity of solving \equref{Equ:MaxTransferDataRecursion} and 
backtracking $\mathcal{J}_{Q_I}$ or $\mathcal{J}_{Q_O}$ is $O(n\times BB)$.


\subsection{Scheduling in the Running Phase}
\label{SubSec:OptRunning}
Running jobs not only require the compute nodes for computation,
but also utilize the burst buffer to enhance the I/O performance.
Here we take a general strategy by defining the \textbf{profit} of $job_i$
in $Q_R$ as it uses $c_i$ compute nodes and $bb\_run_i$ amount of the burst buffer
\begin{equation}
        p_i = f(c_i, bb\_run_i, ert_i)
\label{Equ:GeneralProfit}
\end{equation}
where $ert_i$ is the expected running time of $job_i$.

Similar to the 0-1 knapsack problem, the objective is to maximize the total profit of jobs in the running queue, formularized as
\begin{align*}
        \mathbf{Problem} & \text{ } \mathbf{2} \\
        \max &\sum_{i \in J_{Q_R}} p_i \cdot x_i \\
        s.t. &\left\{
                \begin{array}{l}
                        x_i \in \{0, 1\}, \forall i \in J_{Q_R} \\ [0.8em]
                        \sum_{i \in J_{Q_R}} c_i \cdot x_i \leq CN_{available} \\ [0.8em]\numberthis \label{Equ:MaxProduct} 
                        \sum_{i \in J_{Q_R}} bb\_run_i \cdot x_i \leq BB_{available}
                \end{array} 
        \right.
\end{align*}
where $J_{Q_R}$ is the set of jobs in the running queue,
$CN_{available}$ and $BB_{available}$ represent the amount of system's current available resources.
We denote the set of jobs who have the maximum profit as
$\mathcal{J}  = \{x_i|x_i=1, \text{ } \forall i \in J_{Q_R}\}$.

As an example, we measure the value of a job based on how efficient one utilize its resource:
\begin{equation}
        p_i = \frac{c_i / CN}{ert_i} \times \frac{bb\_run_i / BB}{ert_i}
        \label{Equ:DefValue}
\end{equation}
Here, efficiency is measured as the ratio of the job resource demand to the expected job running time,
where the share of the requested compute nodes ($c_i/CN$) and the burst buffer ($bb\_run/BB$) are used to
represent the demand size.
Specializing Problem 2 using \equref{Equ:DefValue} in fact favors tasks that claim to take up the resources with a short duration, as we will discuss in Section~\ref{Sec:Experiments}.
%\begin{align*}
        %& \max \sum_{i \in S}\frac{c_i}{ert_i} * \frac{bb\_run_i}{ert_i} \\
        %& s.t. \left\{
                %\begin{array}{l}
                        %%S \subseteq J_{Q_R} \\ [1em]
                        %\sum_{i \in S} c_i \leq CN_{available} \\ [0.5em]\numberthis \label{Equ:MaxProduct} 
                        %\sum_{i \in S} bb\_run_i \leq BB_{available}
                %\end{array} 
        %\right.
%\end{align*}


Problem 2 can be informally treat as the two-dimension 0-1 knapsack problem, which is also NP-hard.
We use the same dynamic programming skill to solve Problem 2, the recursion is given by \equref{Equ:MaxProductRecursion}.
The time complexity of solving \equref{Equ:MaxProductRecursion} and selecting $\mathcal{J}_{Q_R}$ with memorization technique is $O(n\times CN\times BB)$.


\subsection{Scheduling in the Stage-out Phase}
\label{SubSec:OptStageOut}

The stage-out phase scheduling is designed based on the value of $bb\_out$ of the jobs in $Q_O$.
The optimization formula for the stage-out phase is similar as in Problem 1, 
except that $bb\_in_i$ is replaced with $bb\_out_i$, 
and $J_{Q_I}$ is replaced with $J_{Q_O}$, the job set of output queue.

%\subsection{Solving the Optimization Problems}
%
%Problem 2 can be informally treat as the 2-dimension 0-1 knapsack problem.
%We expect all of them to be NP-hard problems.
%Dynamic programming is adopted to solve them in pseudo-polynomial time.
%Applying memorization could also help accelerate the solving process.
%In addition, we are not interested in the optimal result of
%Problem 1 and Problem 2,
%but in a combination of jobs $\mathcal{J}$ that yields to the optimal solution,
%which, fortunately, can also be tracked back down using the memorizations stored during the dynamic programming execution.
%
%For Problem 1, the recursive relationship is given by \equref{Equ:MaxTransferDataRecursion}.
%The recursion that solves Problem 2 is given by \equref{Equ:MaxProductRecursion}.
%Solving \equref{Equ:MaxTransferDataRecursion} and 
%backtracking $\mathcal{J}_{Q_I}$ or $\mathcal{J}_{Q_O}$ in total take time proportional to $n\times BB$.
%Similarly, solving \equref{Equ:MaxProductRecursion} and selecting $\mathcal{J}_{Q_R}$ using memo
%are $O(n\times CN\times BB)$.


\begin{strip}
        \begin{align}
                dp(i, w) = & 
                \left\{
                        \begin{array}{l}
                                0, \text{ if $i=0$ } \\ [0.6em]
                                dp(i-1, w), \text{ if $bb\_in_i > w$} \\ [0.6em]
                                \max \{ dp(i-1, w), dp(i-1, w-bb\_in_i) + bb\_in_i \}, \text{ if $bb\_in_i \leq w$}
                        \end{array} 
                \right.
                \label{Equ:MaxTransferDataRecursion} 
        \end{align}
\end{strip}

\begin{strip}
        \begin{align}
                dp(i, c, w) = &
                \left\{
                        \begin{array}{l}
                                0, \text{ if $i=0$ } \\ [0.6em]
                                dp(i-1, c, w), \text{ if $c_i > c$ or $bb\_run_i > w$} \\ [0.6em]
                                \max \{ dp(i-1, c, w), dp(i-1, c - c_i, w - bb\_run_i) + p_i \}, \text{ if $c_i \leq c$ and $bb\_run_i \leq w$}
                        \end{array} 
                \right.
                \label{Equ:MaxProductRecursion}
        \end{align}
\end{strip}







