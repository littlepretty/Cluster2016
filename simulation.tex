% \section{Event-driven Burst Buffer Simulator}
% \label{Sec:Simulation}

% We develop an event-driven simulator for burst buffer enabled HPC system, named \textbf{BBSim},
% from scratch in Python to mimic Cerberus scheduling Trinity.\NOTE{Fig has no info about Trinity}
% It roughly consists of 1,400 lines of code.
% Figure~\ref{Fig:JobFSM} illustrates the simulation lifetime of a job in BBSim
% using extended finite state machine (EFSM).
% The \textit{state} of the job changes depending on current status and
% happening \textit{events}, along with which an \textit{action} is taken.
% For example, at the very beginning, submitted $job_i$ will enter \textbf{Waiting Stage-in} state;
% at the meanwhile, $job_i$ is enqueued to $Q_I$ and scheduler is triggered to do scheduling.

%=========XY===========
\section{BBSim: An Event Driven Simulator of Cerberus}
\label{Sec:Simulation}

We develop an event-driven simulator, named \textbf{BBSim},
to simulate how Cerberus schedules jobs on burst-buffer-enabled HPC systems.
Besides demonstrating the lifetime of a job scheduled by Cerberus,
Figure~\ref{Fig:JobFSM} also illustrates the various events in BBSim and how to handle
them in an event-driven-simulation approach.
The changes of job \textit{state} depend on the current status and the \textit{events} to trigger \textit{actions}.
For example, when $job_i$ is submitted to the system,
it enters the \textit{Waiting Stage-in} state,
waiting to be scheduled in the job queue $Q_I$. 
The scheduler checks $job_i$'s resource demand and
dispatches it to run when sufficient resources are available.

% Whenever job enters one of its 3 phases, system resources are allocated:
% \begin{itemize}
%         \item $BB_{in}$ TB amount of burst buffer are allocated upon entering stage-in phase.
%         \item $CN$ number of compute nodes and $BB_{run}$ amount of burst buffer
%                 are allocated upon entering running phase.
%         \item $BB_{out}$ TB of burst buffer are allocated upon entering stage-out phase.
% \end{itemize}
% Various \texttt{release()} actions are of importance because, in addition to submission,
% \texttt{schedule()} is invoked to schedule waiting jobs
% whenever system resources are released:
% \begin{itemize}
%         \item When job's data is loaded from burst buffer to memory,
%                 burst buffer allocated at stage-in phase are released.
%         \item When job finishes running, system reclaims burst buffer nodes
%                 used for checkpointing.
%         \item When job's data is written to burst buffer from memory,
%                 compute nodes taken by job are returned.
%         \item When job's data is staged out to external disk,
%                 its burst buffer nodes are released.
% \end{itemize}
% 
% \begin{figure}[!t]
% \centering
%         \includegraphics[width=3.6in]{3PhaseJobFSM}
%         \caption{Finite State Machine of Scheduling Simulation}
% \label{Fig:JobFSM}
% \end{figure}


%Whenever $job_i$ enters one of the three phases, the system resources are allocated in the following way:
%\begin{itemize}
        %\item $BB_{in}$ TB amount of burst buffer are allocated upon entering the stage-in phase.
        %\item $CN$ number of compute nodes and $BB_{run}$ amount of burst buffer
                %are allocated upon entering the running phase.
        %\item $BB_{out}$ TB of burst buffer is allocated upon entering the stage-out phase.
%\end{itemize}

Various \texttt{release()} actions are important because, in addition to job submissions,
\texttt{schedule()} is invoked to schedule the waiting jobs.
The allocated resource can only be released when one phase is finished.
Therefore, any \textit{dispatch} event, generated by the \texttt{schedule()} action,
follows a certain \textit{finish} event.
The allocated resources are released at the following time points:
\begin{itemize}
        \item When $job_i$ pre-fetches data from the burst buffer to the memory,
                the burst buffer allocated in the stage-in phase ($BB_{in}$) is released.
        \item When $job_i$ completes the computation,
                the system reclaims the burst buffer allocated in the running phase ($BB_{run}$).
        \item When $job_i$ output data is written to the burst buffer from the memory,
              compute nodes ($CN$) allocated in the running phase are released.
        \item When output data are drained out to the external storage,
                the burst buffer allocated in the stage-out phase ($BB_{out}$) is released.
                The simulation for $job_i$ completes.
\end{itemize}




% Notice that resource can only be released when certain phase is finished.
% Therefore, any \textit{dispatch} event, caused by \texttt{schedule()} action, actually happens
% at the meanwhile of a certain \textit{finish} event.
% The timestamps of all possible \textit{finish} events are calculated in the following way:
% \begin{align}
%         & TS_{f\_stagein} = TS_{s\_stagein} + \frac{bb\_in}{BW_{io\_to\_bb}}\label{Equ:FinIn} \\
%         & TS_{f\_loadin} = TS_{s\_run} + \frac{bb\_in}{BW_{bb\_to\_cn}}\label{Equ:FinMemIn} \\
%         & TS_{f\_run} = TS_{f\_loadin} + \frac{bb\_run}{BW_{cn\_to\_bb}} + rt\label{Equ:FinRun} \\
%         & TS_{f\_loadout} = TS_{s\_stageout} + \frac{bb\_out}{BW_{cn\_to\_bb}}\label{Equ:FinMemOut} \\
%         & TS_{f\_stageout} = TS_{f\_loadout} + \frac{bb\_out}{BW_{bb\_to\_io}} \label{Equ:FinOut}
% \end{align}
% where $TS_{f\_x}$ stands for the timestamps of finishing phase $x$,
% $TS_{s\_x}$ stands for the timestamps of starting phase $x$,
% $BW_{x\_to\_y}$ stands for the bandwidth between $x$ and $y$.
% 
% Though target on burst buffer system, BBSim also supports simulating non-BB HPC system.
% Besides, it is not coupled with Cerberus but easy to simulating many other scheduler.
% Both can be proved by the following experiments for various schedulers.


%==========XY============

%The timestamps of all the \textit{finish} events are calculated as follows:
%\begin{align}
%        & TS_{f\_stagein} = TS_{s\_stagein} + \frac{bb\_in}{BW_{io\_to\_bb}}\label{Equ:FinIn} \\
%        & TS_{f\_loadin} = TS_{s\_run} + \frac{bb\_in}{BW_{bb\_to\_cn}}\label{Equ:FinMemIn} \\
%        & TS_{f\_run} = TS_{f\_loadin} + \frac{bb\_run}{BW_{cn\_to\_bb}} + rt\label{Equ:FinRun} \\
%        & TS_{f\_loadout} = TS_{s\_stageout} + \frac{bb\_out}{BW_{cn\_to\_bb}}\label{Equ:FinMemOut} \\
%        & TS_{f\_stageout} = TS_{f\_loadout} + \frac{bb\_out}{BW_{bb\_to\_io}} \label{Equ:FinOut}
%\end{align}
%where $TS_{f\_x}$ stands for the timestamps of the finishing phase $x$,
%$TS_{s\_x}$ stands for the timestamps of the starting phase $x$,
%$BW_{x\_to\_y}$ stands for the bandwidth between $x$ and $y$.
%BBSim does not simulate the data transfer between IO nodes and PFS through the network.
The time spent for I/O in our simulation is calculated by an analytical model, which is 
\begin{align}
        & \text{Time}_{\text{I/O}} =\frac{\text{Data Amount}}{\text{Link Bandwidth}}\label{Equ:IOTime}
\end{align}
Though targeting on burst buffer enabled systems,
BBSim also supports simulating job scheduling on HPC systems without burst buffer.
Besides, it is easy to integrate other scheduling policies into BBsim.
The codebase of BBsim and Cerberus are made public to the community\cite{bbsim-github}.



% % system
% We consider simulating the full Trinity super computer\cite{TrinitySystem}.
% The number of compute nodes on Trinity is about 18,936,
% e.g. 9,436 Intel Haswell nodes
% and at least 9500 Intel Xeon Phi nodes.
% There are 16 cores on each processor, thus totally 302,976 cores.
% In the following experiments, we compare two identical system except that
% IO nodes are replaced by the same number of burst buffer nodes.
% Eventually Trinity plans to delivery up to 576 burst buffer nodes of 3.7 PB,
% consisted of Trinity IO nodes with PCIe SSD card.
% They are globally accessible as intermediate storage and distributed among cabinets.
% Sequential read/write speed of burst buffer nodes is 8.0 GBps.
% Bandwidth between CPU node and non-burst-buffer IO node is set to 2.5 GBps.


BBsim simulates a burst-buffer-enabled HPC system, as shown in Figure~\ref{Fig:BBArchitecture}.
This system architecture is inspired by Trinity~\cite{TrinitySystem}, 
the next generation supercomputer in Los Alamos National Laboratory.
The parameters used in our simulation are taken from the 
Trinity/NERSC-8 Use Case Scenarios technical report\cite{BBUseCase}.
The simulated system consists of 18,936 compute nodes,
e.g., 9,436 Intel Haswell nodes and 9,500 Intel Xeon Phi nodes.
There are 576 burst buffer nodes with an aggregated 3.7 PB storage, which are globally accessible as the intermediate storage.
The sequential read/write speed of burst buffer nodes is 8.0 GBps.
The bandwidth between a CPU node and an IO node is set to 2.5 GBps.

\begin{figure}[htp]
        \centering
        \includegraphics[width=3.5in]{BBArchitecturewithBandwidth}
        \caption{Our simulated burst buffer enabled HPC system is inspired by Trinity at LANL. The bandwidth between compute nodes and burst buffer node is 8.0 GBps. The bandwidth between compute nodes and I/O nodes is 2.5 GBps.}
        \label{Fig:BBArchitecture}
\end{figure}

% % trace
% Job trace is from ANL's Blue Gene Intrepid system,
% containing totally 68,936 jobs run during January to September 2009\cite{JobTrace}.
% We extract two critical fields from this jobs trace: running time and
% number of cores user requested.
% In this section we take a window of 1,185 jobs and report their scheduling results.
% We patched 3 fields to each job's log entry: the amount of input data $data\_in$,
% the total amount of written data for checkpointings $data\_run$
% and the amount of output data $data\_out$.
% We assume $data\_run$ and $data\_out$ follows uniform distribution with
% lower boundary of 1 TiB and upper boundary of 60 TiB;
% $data\_in$ follows uniform distribution between 1 GiB and 30 Gib.
% The patches 3 fields may or may not be used in scheduling,
% depends on both the model of the jobs and the experiment scenario.


Since Trinity is not available to use at present, 
there is no Trinity job log available for our study.
We use the job log collected from ANL's Blue Gene Intrepid system. The log contains 68,936 jobs
submitted to the system from January to September 2009~\cite{Tang:IPDPS:2010}.
We patched 3 fields to each job's log entry: the amount of input data $data\_in$,
the total amount of written data for checkpointings $data\_run$
and the amount of output data $data\_out$.
We assume $data\_run$ and $data\_out$ follows the uniform distribution with a
lower bound of 1 TB and a upper bound of 60 TB;
$data\_in$ follows the uniform distribution between 1 GB and 30 GB~\cite{Liu:MSST:2012}.




